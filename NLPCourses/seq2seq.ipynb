{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder : Context Vector를 만들어주고, \n",
    "# Decoder : Vector를 출력 시퀀스로 만들어줌.\n",
    "\n",
    "# Transformer 도 동일함.\n",
    "\n",
    "# Seq2Seq : LSTM + LSTM \n",
    "# Transformer : DNN + Attention\n",
    "\n",
    "\n",
    "# -> 학습과 예측의 방식이 다르다. 학습할 때는 디코더 입력 문장을 알고 있지만, 예측할 때는 인코더 입력 문장만 알게 된다.\n",
    "# -> 얘측하는 것으로 하면 다음 시점에 입력으로 주어야 한다. (예측은 Auto-regressive 방식으로 동작. -> 생성 AI)\n",
    "# 학습 초기에 문제 -> 나올 수 있는 모든 단어들의 확률이 비슷하다. -> 학습은 Teacher Forcing (출력을 가정하고 입력을 넣음.)\n",
    "\n",
    "\n",
    "# 기계번역의 평가 메트릭 -> 번역하는 사람마다 조금씩 다르다.\n",
    "# BLEU 스코어 : 번역에 대한 정답 문장은 여러 개일 수 있다는 가정을 한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 매커니즘"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
