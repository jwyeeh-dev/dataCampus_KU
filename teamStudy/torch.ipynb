{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.backends.mps.is_built())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.backends.mps.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/jwyeeh/miniforge3/envs/pytorch/lib/python3.8/site-packages (2.0.0)\n",
      "Collecting torch\n",
      "  Downloading torch-2.0.1-cp38-none-macosx_11_0_arm64.whl (55.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchaudio in /Users/jwyeeh/miniforge3/envs/pytorch/lib/python3.8/site-packages (2.0.1)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.0.2-cp38-cp38-macosx_11_0_arm64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /Users/jwyeeh/miniforge3/envs/pytorch/lib/python3.8/site-packages (0.15.1)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.15.2-cp38-cp38-macosx_11_0_arm64.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /Users/jwyeeh/miniforge3/envs/pytorch/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/jwyeeh/miniforge3/envs/pytorch/lib/python3.8/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: sympy in /Users/jwyeeh/miniforge3/envs/pytorch/lib/python3.8/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: filelock in /Users/jwyeeh/miniforge3/envs/pytorch/lib/python3.8/site-packages (from torch) (3.10.7)\n",
      "Requirement already satisfied: typing-extensions in /Users/jwyeeh/miniforge3/envs/pytorch/lib/python3.8/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: requests in /Users/jwyeeh/miniforge3/envs/pytorch/lib/python3.8/site-packages (from torchvision) (2.28.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/jwyeeh/miniforge3/envs/pytorch/lib/python3.8/site-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: numpy in /Users/jwyeeh/miniforge3/envs/pytorch/lib/python3.8/site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jwyeeh/miniforge3/envs/pytorch/lib/python3.8/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jwyeeh/miniforge3/envs/pytorch/lib/python3.8/site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jwyeeh/miniforge3/envs/pytorch/lib/python3.8/site-packages (from requests->torchvision) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jwyeeh/miniforge3/envs/pytorch/lib/python3.8/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/jwyeeh/miniforge3/envs/pytorch/lib/python3.8/site-packages (from requests->torchvision) (1.26.15)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/jwyeeh/miniforge3/envs/pytorch/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.0\n",
      "    Uninstalling torch-2.0.0:\n",
      "      Successfully uninstalled torch-2.0.0\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.15.1\n",
      "    Uninstalling torchvision-0.15.1:\n",
      "      Successfully uninstalled torchvision-0.15.1\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.0.1\n",
      "    Uninstalling torchaudio-2.0.1:\n",
      "      Successfully uninstalled torchaudio-2.0.1\n",
      "Successfully installed torch-2.0.1 torchaudio-2.0.2 torchvision-0.15.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U torch torchaudio torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                     /Users/jwyeeh/miniforge3\n",
      "pytorch               *  /Users/jwyeeh/miniforge3/envs/pytorch\n",
      "tf                       /Users/jwyeeh/miniforge3/envs/tf\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transfroms\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps is available\n"
     ]
    }
   ],
   "source": [
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "torch.manual_seed(777)\n",
    "if device == 'mps':\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "print(device + \" is available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "num_classes = 10\n",
    "epochs = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MNIST 데이터셋 로드\n",
    "train_set = torchvision.datasets.MNIST(\n",
    "    root = './data/MNIST',\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transfroms.Compose([\n",
    "        transfroms.ToTensor() # 데이터를 0에서 255까지 있는 값을 0에서 1사이 값으로 변환\n",
    "    ])\n",
    ")\n",
    "test_set = torchvision.datasets.MNIST(\n",
    "    root = './data/MNIST',\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = transfroms.Compose([\n",
    "        transfroms.ToTensor() # 데이터를 0에서 255까지 있는 값을 0에서 1사이 값으로 변환\n",
    "    ])\n",
    ")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_loader, test_loader 생성\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)\n",
    " \n",
    "# input size를 알기 위해서\n",
    "examples = enumerate(train_set)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "example_data.shape\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "  def __init__(self): # layer 정의\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        # input size = 28x28 \n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5) # input channel = 1, filter = 10, kernel size = 5, zero padding = 0, stribe = 1\n",
    "        # ((W-K+2P)/S)+1 공식으로 인해 ((28-5+0)/1)+1=24 -> 24x24로 변환\n",
    "        # maxpooling하면 12x12\n",
    "  \n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5) # input channel = 1, filter = 10, kernel size = 5, zero padding = 0, stribe = 1\n",
    "        # ((12-5+0)/1)+1=8 -> 8x8로 변환\n",
    "        # maxpooling하면 4x4\n",
    "\n",
    "        self.drop2D = nn.Dropout2d(p=0.25, inplace=False) # 랜덤하게 뉴런을 종료해서 학습을 방해해 학습이 학습용 데이터에 치우치는 현상을 막기 위해 사용\n",
    "        self.mp = nn.MaxPool2d(2)  # 오버피팅을 방지하고, 연산에 들어가는 자원을 줄이기 위해 maxpolling\n",
    "        self.fc1 = nn.Linear(320,100) # 4x4x20 vector로 flat한 것을 100개의 출력으로 변경\n",
    "        self.fc2 = nn.Linear(100,10) # 100개의 출력을 10개의 출력으로 변경\n",
    "\n",
    "  def forward(self, x):\n",
    "        x = F.relu(self.mp(self.conv1(x))) # convolution layer 1번에 relu를 씌우고 maxpool, 결과값은 12x12x10\n",
    "        x = F.relu(self.mp(self.conv2(x))) # convolution layer 2번에 relu를 씌우고 maxpool, 결과값은 4x4x20\n",
    "        x = self.drop2D(x)\n",
    "        x = x.view(x.size(0), -1) # flat\n",
    "        x = self.fc1(x) # fc1 레이어에 삽입\n",
    "        x = self.fc2(x) # fc2 레이어에 삽입\n",
    "        return F.log_softmax(x) # fully-connected layer에 넣고 logsoftmax 적용\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s1/6z_vw3ln3nd621_2gd81gzr40000gn/T/ipykernel_30908/3752254703.py:26: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x) # fully-connected layer에 넣고 logsoftmax 적용\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    1] cost = 0.317633063\n",
      "[Epoch:    2] cost = 0.117046542\n",
      "[Epoch:    3] cost = 0.0905887187\n",
      "[Epoch:    4] cost = 0.0754274204\n",
      "[Epoch:    5] cost = 0.0661753938\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet().to(device) # CNN instance 생성\n",
    "# Cost Function과 Optimizer 선택\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    " \n",
    "for epoch in range(epochs): # epochs수만큼 반복\n",
    "    avg_cost = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad() # 모든 model의 gradient 값을 0으로 설정\n",
    "        hypothesis = model(data) # 모델을 forward pass해 결과값 저장 \n",
    "        cost = criterion(hypothesis, target) # output과 target의 loss 계산\n",
    "        cost.backward() # backward 함수를 호출해 gradient 계산\n",
    "        optimizer.step() # 모델의 학습 파라미터 갱신\n",
    "        avg_cost += cost / len(train_loader) # loss 값을 변수에 누적하고 train_loader의 개수로 나눔 = 평균\n",
    "    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s1/6z_vw3ln3nd621_2gd81gzr40000gn/T/ipykernel_30908/3752254703.py:26: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x) # fully-connected layer에 넣고 logsoftmax 적용\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  98.51 %\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "model.eval() # evaluate mode로 전환 dropout 이나 batch_normalization 해제 \n",
    "with torch.no_grad(): # grad 해제 \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        out = model(data)\n",
    "        preds = torch.max(out.data, 1)[1] # 출력이 분류 각각에 대한 값으로 나타나기 때문에, 가장 높은 값을 갖는 인덱스를 추출\n",
    "        total += len(target) # 전체 클래스 개수 \n",
    "        correct += (preds==target).sum().item() # 예측값과 실제값이 같은지 비교\n",
    "        \n",
    "    print('Test Accuracy: ', 100.*correct/total, '%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
